{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1653149a",
   "metadata": {},
   "source": [
    "# Badis — Notebook d'entraînement DKT **enrichi**\n",
    "Ce notebook entraîne un modèle **Deep Knowledge Tracing (DKT)** enrichi par des signaux contextuels (difficulté, ajustements ZPDES, temps de réponse, motivation, anxiété, phase).\n",
    "\n",
    "## Points clés\n",
    "- Schéma d'interaction unique: `(skill_id, correct, difficulty, adjustment, time_spent, motivation, anxiety_level, phase_type)`\n",
    "- Encodage étendu pour DKT: concaténation de one-hot (`skill_id`, `phase_type`) + variables continues\n",
    "- Modèle: GRU → projection sigmoïde (probabilités de réussite par compétence)\n",
    "- Perte: **BCE** sur la probabilité de réussite du **pas suivant** pour la compétence réellement tentée\n",
    "- Métriques: AUC, Accuracy\n",
    "- Visualisations: courbes d'entraînement (loss, AUC)\n",
    "- Données: générateur synthétique **ou** chargement d'un CSV externe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2fc54",
   "metadata": {},
   "source": [
    "## 1) Installation & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab24bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si nécessaire, décommentez pour installer des dépendances\n",
    "# !pip install scikit-learn torch pandas numpy matplotlib\n",
    "\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099efd55",
   "metadata": {},
   "source": [
    "## 2) Schéma des données & phases\n",
    "Nous utilisons un enregistrement **unique** par interaction:  \n",
    "`(skill_id, correct, difficulty, adjustment, time_spent, motivation, anxiety_level, phase_type)`\n",
    "\n",
    "- `skill_id` : entier dans `[0..K-1]`  \n",
    "- `correct` : {0,1}  \n",
    "- `difficulty` : flottant ∈ [0.2, 0.9]  \n",
    "- `adjustment` : {-1, 0, +1} (ajustement de ZPDES)  \n",
    "- `time_spent` : temps de réponse (sec)  \n",
    "- `motivation` : [0,1]  \n",
    "- `anxiety_level` : [0,1]  \n",
    "- `phase_type` : dans `{'hub','exploration','lecon','synthese','defi'}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0213cc",
   "metadata": {},
   "source": [
    "## 3) Générateur de données synthétiques (+ règles ZPDES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASES = ['hub','exploration','lecon','synthese','defi']\n",
    "\n",
    "def zpdes_adjust(difficulty: float, consec_fail: int, consec_succ: int):\n",
    "    \"\"\"Règle simple (inspirée ZPDES) pour ajuster la difficulté en temps réel.\n",
    "    - Si >2 échecs consécutifs → baisse la difficulté\n",
    "    - Si >3 succès consécutifs → augmente la difficulté\n",
    "    - Bornes: [0.2, 0.9]\n",
    "    Retourne (new_difficulty, adjustment)\n",
    "    \"\"\"\n",
    "    adjustment = 0\n",
    "    if consec_fail > 2:\n",
    "        adjustment = -1\n",
    "    elif consec_succ > 3:\n",
    "        adjustment = +1\n",
    "    new_diff = min(0.9, max(0.2, difficulty + 0.1 * adjustment))\n",
    "    return new_diff, adjustment\n",
    "\n",
    "def generate_synthetic_classroom(\n",
    "    num_students=120, num_skills=12, max_seq_len=80, seed=42\n",
    "):\n",
    "    \"\"\"Génère un DataFrame d'interactions avec le schéma enrichi.\n",
    "    Hypothèses simplifiées:\n",
    "    - Chaque étudiant a une maîtrise latente par compétence (beta)\n",
    "    - La probabilité de réussite dépend de la maîtrise, de la difficulté et de l'anxiété\n",
    "    - Un mini-contrôle ZPDES ajuste la difficulté selon les réussites/échecs récents\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    rows = []\n",
    "    for s in range(num_students):\n",
    "        # Profil élève\n",
    "        mastery = np.clip(np.random.normal(0.0, 0.6, size=num_skills), -1.5, 1.5)\n",
    "        motivation = np.clip(np.random.beta(2, 2), 0, 1)\n",
    "        anxiety_base = np.clip(np.random.beta(2, 5), 0, 1)\n",
    "\n",
    "        # séquence individuelle\n",
    "        seq_len = rng.randint(max_seq_len//2, max_seq_len)\n",
    "        diff = rng.uniform(0.3, 0.7)\n",
    "        c_fail = 0\n",
    "        c_succ = 0\n",
    "        for t in range(seq_len):\n",
    "            skill = rng.randrange(num_skills)\n",
    "            phase = rng.choice(PHASES)\n",
    "\n",
    "            # anxiété contextuelle (augmente avec les échecs consécutifs)\n",
    "            anxiety = np.clip(anxiety_base + 0.05*c_fail - 0.03*c_succ, 0, 1)\n",
    "            # temps de réponse: plus haut si difficulté/anxiété élevées\n",
    "            time_spent = max(0.5, np.random.lognormal(mean=math.log(2 + 6*diff + 3*anxiety), sigma=0.4))\n",
    "            # prob réussite (logit simple)\n",
    "            logit = mastery[skill] - (diff - 0.5)*2.0 - (anxiety - 0.3)*1.2\n",
    "            p_correct = 1 / (1 + math.exp(-logit))\n",
    "            correct = 1 if rng.random() < p_correct else 0\n",
    "\n",
    "            # MàJ compteurs\n",
    "            if correct:\n",
    "                c_succ += 1\n",
    "                c_fail = 0\n",
    "                # légère progression de maîtrise\n",
    "                mastery[skill] = np.clip(mastery[skill] + 0.05*(0.8 - diff), -2, 2)\n",
    "            else:\n",
    "                c_fail += 1\n",
    "                c_succ = 0\n",
    "                # légère baisse de maîtrise perçue\n",
    "                mastery[skill] = np.clip(mastery[skill] - 0.03*(diff + anxiety), -2, 2)\n",
    "\n",
    "            # ZPDES adjust\n",
    "            new_diff, adjust = zpdes_adjust(diff, c_fail, c_succ)\n",
    "            diff = new_diff\n",
    "\n",
    "            rows.append({\n",
    "                \"student_id\": s,\n",
    "                \"t\": t,\n",
    "                \"skill_id\": skill,\n",
    "                \"correct\": correct,\n",
    "                \"difficulty\": round(diff, 3),\n",
    "                \"adjustment\": adjust,\n",
    "                \"time_spent\": float(time_spent),\n",
    "                \"motivation\": float(motivation),\n",
    "                \"anxiety_level\": float(anxiety),\n",
    "                \"phase_type\": phase\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "df = generate_synthetic_classroom(num_students=2500, num_skills=5, max_seq_len=100, seed=7)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.shape)\n",
    "print(df.describe(include='all').T.iloc[:10])\n",
    "df['phase_type'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67ecd7",
   "metadata": {},
   "source": [
    "### (Option) Charger vos propres données\n",
    "Le CSV doit contenir **au minimum** ces colonnes:\n",
    "- `student_id, t, skill_id, correct, difficulty, adjustment, time_spent, motivation, anxiety_level, phase_type`\n",
    "\n",
    "Décommentez et ajustez le chemin si vous avez un fichier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('models/mes_traces.csv')  \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bb843",
   "metadata": {},
   "source": [
    "## 4) Encodage des features (one-hot + continues normalisées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f736ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def build_encoders(df: pd.DataFrame):\n",
    "    ohe_skill = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    ohe_phase = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    ohe_skill.fit(df[['skill_id']])\n",
    "    ohe_phase.fit(df[['phase_type']])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cont = df[['difficulty','adjustment','time_spent','motivation','anxiety_level']].astype(float)\n",
    "    scaler.fit(cont)\n",
    "\n",
    "    return ohe_skill, ohe_phase, scaler\n",
    "\n",
    "ohe_skill, ohe_phase, scaler = build_encoders(df)\n",
    "\n",
    "num_skills = len(ohe_skill.categories_[0])\n",
    "num_phases = len(ohe_phase.categories_[0])\n",
    "feature_dim = num_skills + num_phases + 5  # 5 continuous features\n",
    "num_skills, num_phases, feature_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f822f3",
   "metadata": {},
   "source": [
    "## 5) Dataset séquentiel (par élève)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_row(row, ohe_skill, ohe_phase, scaler):\n",
    "    skill_oh = ohe_skill.transform([[row['skill_id']]])[0]\n",
    "    phase_oh  = ohe_phase.transform([[row['phase_type']]])[0]\n",
    "    cont = np.array([[row['difficulty'], row['adjustment'], row['time_spent'], row['motivation'], row['anxiety_level']]], dtype=float)\n",
    "    cont_scaled = scaler.transform(cont)[0]\n",
    "    x = np.concatenate([skill_oh, phase_oh, cont_scaled], axis=0).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "class DKTSequenceDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, ohe_skill, ohe_phase, scaler, num_skills: int, seq_len_cap: int = 150):\n",
    "        self.num_skills = num_skills\n",
    "        self.ohe_skill = ohe_skill\n",
    "        self.ohe_phase = ohe_phase\n",
    "        self.scaler = scaler\n",
    "        self.seq_len_cap = seq_len_cap\n",
    "\n",
    "        # group by student and sort by t\n",
    "        grouped = []\n",
    "        for sid, g in df.sort_values(['student_id','t']).groupby('student_id'):\n",
    "            g = g.reset_index(drop=True)\n",
    "            # cap sequence length (take last seq_len_cap interactions to keep \"recent\" signal)\n",
    "            if len(g) > seq_len_cap:\n",
    "                g = g.iloc[-seq_len_cap:].reset_index(drop=True)\n",
    "            # build tensors\n",
    "            X = np.stack([encode_row(g.iloc[i], ohe_skill, ohe_phase, scaler) for i in range(len(g))])\n",
    "            # next-step targets: for t, predict correctness at t+1\n",
    "            # also need \"next skill\" indices to select correct head from multi-skill output\n",
    "            next_correct = np.zeros(len(g), dtype=np.float32)\n",
    "            next_skill = np.zeros(len(g), dtype=np.int64)\n",
    "            for i in range(len(g)-1):\n",
    "                next_correct[i] = g.loc[i+1, 'correct']\n",
    "                next_skill[i]   = int(g.loc[i+1, 'skill_id'])\n",
    "            next_correct[-1] = g.loc[len(g)-1, 'correct']  # tail (won't be used if masked)\n",
    "            next_skill[-1]   = int(g.loc[len(g)-1, 'skill_id'])\n",
    "            mask = np.ones(len(g), dtype=np.float32)\n",
    "            mask[-1] = 0.0  # typically don't score last step since no next action\n",
    "\n",
    "            grouped.append((X, next_skill, next_correct, mask))\n",
    "\n",
    "        self.samples = grouped\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, next_skill, next_correct, mask = self.samples[idx]\n",
    "        return torch.from_numpy(X), torch.from_numpy(next_skill), torch.from_numpy(next_correct), torch.from_numpy(mask)\n",
    "\n",
    "dataset = DKTSequenceDataset(df, ohe_skill, ohe_phase, scaler, num_skills=num_skills, seq_len_cap=150)\n",
    "len(dataset), dataset[0][0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697a549",
   "metadata": {},
   "source": [
    "## 6) Split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ea7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset))\n",
    "np.random.shuffle(indices)\n",
    "n = len(indices)\n",
    "train_idx = indices[:int(0.7*n)]\n",
    "val_idx   = indices[int(0.7*n):int(0.85*n)]\n",
    "test_idx  = indices[int(0.85*n):]\n",
    "\n",
    "def subset(dataset, idxs):\n",
    "    class _Subset(Dataset):\n",
    "        def __init__(self, base, idxs):\n",
    "            self.base = base\n",
    "            self.idxs = idxs\n",
    "        def __len__(self): return len(self.idxs)\n",
    "        def __getitem__(self, i): return self.base[self.idxs[i]]\n",
    "    return _Subset(dataset, idxs)\n",
    "\n",
    "train_ds = subset(dataset, train_idx)\n",
    "val_ds   = subset(dataset, val_idx)\n",
    "test_ds  = subset(dataset, test_idx)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def collate(batch):\n",
    "    # variable-length handling via padding to max_len within batch\n",
    "    Xs, ks, ys, ms = zip(*batch)\n",
    "    lengths = [x.shape[0] for x in Xs]\n",
    "    max_len = max(lengths)\n",
    "\n",
    "    X_pad = torch.zeros(len(batch), max_len, Xs[0].shape[1], dtype=torch.float32)\n",
    "    k_pad = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    y_pad = torch.zeros(len(batch), max_len, dtype=torch.float32)\n",
    "    m_pad = torch.zeros(len(batch), max_len, dtype=torch.float32)\n",
    "\n",
    "    for i,(x,k,y,m) in enumerate(batch):\n",
    "        L = x.shape[0]\n",
    "        X_pad[i,:L] = x\n",
    "        k_pad[i,:L] = k\n",
    "        y_pad[i,:L] = y\n",
    "        m_pad[i,:L] = m\n",
    "\n",
    "    return X_pad.to(device), k_pad.to(device), y_pad.to(device), m_pad.to(device), torch.tensor(lengths, dtype=torch.long).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc2812",
   "metadata": {},
   "source": [
    "## 7) Modèle DKT enrichi (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKTEnriched(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_skills, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.out = nn.Linear(hidden_dim, num_skills)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        # pack for efficiency\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(X, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out_packed, h = self.gru(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "        logits = self.out(out)  # [B, T, K]\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs  # prob of correctness for each skill at each step\n",
    "\n",
    "model = DKTEnriched(input_dim=feature_dim, hidden_dim=128, num_skills=num_skills, num_layers=1, dropout=0.1).to(device)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainables = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total: {total/1e6:.3f} M params  ({total:,})\")\n",
    "print(f\"Apprenables: {trainables/1e6:.3f} M params\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cce064",
   "metadata": {},
   "source": [
    "## 8) Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e40d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_epoch(model, loader, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    if is_train: model.train()\n",
    "    else: model.eval()\n",
    "\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_tgts = []\n",
    "\n",
    "    bce = nn.BCELoss(reduction='none')\n",
    "\n",
    "    for X, k, y, m, lengths in loader:\n",
    "        probs = model(X, lengths)  # [B,T,K]\n",
    "        B,T,K = probs.shape\n",
    "\n",
    "        # sélectionner la tête correspondant à la \"next skill\" (k)\n",
    "        # gather over last dimension K\n",
    "        idx = k.unsqueeze(-1)  # [B,T,1]\n",
    "        sel = torch.gather(probs, dim=-1, index=idx).squeeze(-1)  # [B,T]\n",
    "\n",
    "        # BCE + masque\n",
    "        loss_mat = bce(sel, y)\n",
    "        loss = (loss_mat * m).sum() / (m.sum() + 1e-8)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Stocker prédictions/targets masqués pour métriques\n",
    "        mask_flat = m.detach().cpu().numpy().reshape(-1) > 0.5\n",
    "        preds_flat = sel.detach().cpu().numpy().reshape(-1)\n",
    "        tgts_flat  = y.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "        all_preds.append(preds_flat[mask_flat])\n",
    "        all_tgts.append(tgts_flat[mask_flat])\n",
    "\n",
    "    if len(all_preds) == 0:\n",
    "        return np.mean(losses), float('nan'), float('nan')\n",
    "\n",
    "    preds = np.concatenate(all_preds)\n",
    "    tgts = np.concatenate(all_tgts)\n",
    "\n",
    "    # AUC peut échouer si tgts n'a qu'une classe\n",
    "    try:\n",
    "        auc = roc_auc_score(tgts, preds)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "\n",
    "    acc = accuracy_score(tgts >= 0.5, preds >= 0.5)\n",
    "\n",
    "    return np.mean(losses), auc, acc\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 12\n",
    "history = {\"train_loss\":[], \"val_loss\":[], \"train_auc\":[], \"val_auc\":[], \"train_acc\":[], \"val_acc\":[]}\n",
    "best_val = float('inf')\n",
    "patience, bad = 4, 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_auc, tr_acc = step_epoch(model, train_loader, optimizer=optimizer)\n",
    "    vl_loss, vl_auc, vl_acc = step_epoch(model, val_loader, optimizer=None)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"val_loss\"].append(vl_loss)\n",
    "    history[\"train_auc\"].append(tr_auc)\n",
    "    history[\"val_auc\"].append(vl_auc)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_acc\"].append(vl_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={tr_loss:.4f} val_loss={vl_loss:.4f} | train_auc={tr_auc:.3f} val_auc={vl_auc:.3f} | train_acc={tr_acc:.3f} val_acc={vl_acc:.3f}\")\n",
    "\n",
    "    if vl_loss < best_val - 1e-4:\n",
    "        best_val = vl_loss\n",
    "        best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139970e",
   "metadata": {},
   "source": [
    "## 9) Courbes d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5953f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history['train_loss'], label='train_loss')\n",
    "plt.plot(history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history['train_auc'], label='train_auc')\n",
    "plt.plot(history['val_auc'], label='val_auc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04d0f1",
   "metadata": {},
   "source": [
    "## 10) Évaluation finale sur test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_auc, test_acc = step_epoch(model, test_loader, optimizer=None)\n",
    "print({\"test_loss\": test_loss, \"test_auc\": test_auc, \"test_acc\": test_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e30362",
   "metadata": {},
   "source": [
    "## 11) Sauvegarde du modèle & des encodeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/dkt_enriched.pt'\n",
    "ENC_INFO_PATH = 'models/dkt_enriched_encoders.npz'\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "# Sauvegarder encoders/scaler (paramètres nécessaires pour l'inférence)\n",
    "np.savez(ENC_INFO_PATH,\n",
    "         skill_categories=ohe_skill.categories_[0],\n",
    "         phase_categories=ohe_phase.categories_[0],\n",
    "         scaler_mean=scaler.mean_,\n",
    "         scaler_scale=scaler.scale_)\n",
    "\n",
    "print('Saved:', MODEL_PATH, ENC_INFO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22b187",
   "metadata": {},
   "source": [
    "## 12) Démo d'inférence (séquence courte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8499871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire une mini-séquence (3 interactions) et inférer la proba de réussite par compétence au pas suivant\n",
    "sample = df[df['student_id']==0].sort_values('t').head(5).copy()\n",
    "X_seq = np.stack([encode_row(sample.iloc[i], ohe_skill, ohe_phase, scaler) for i in range(len(sample))])\n",
    "X_seq = torch.from_numpy(X_seq).unsqueeze(0).to(device)\n",
    "lengths = torch.tensor([X_seq.shape[1]]).to(device)\n",
    "with torch.no_grad():\n",
    "    probs = model(X_seq, lengths)  # [1,T,K]\n",
    "probs[:, -1, :].cpu().numpy()[0][:10]  # afficher les 10 premières compétences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b4423",
   "metadata": {},
   "source": [
    "## 13) Export optionnel des données synthétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'models/dkt_synth_data.csv'\n",
    "df.to_csv(DATA_PATH, index=False)\n",
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b999b",
   "metadata": {},
   "source": [
    "## 14) Modèle de CSV (template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_PATH = 'models/dkt_template.csv'\n",
    "template = pd.DataFrame({\n",
    "    'student_id': [0,0,0],\n",
    "    't': [0,1,2],\n",
    "    'skill_id': [0,1,0],\n",
    "    'correct': [1,0,1],\n",
    "    'difficulty': [0.5,0.6,0.55],\n",
    "    'adjustment': [0,-1,0],\n",
    "    'time_spent': [3.2,5.1,2.7],\n",
    "    'motivation': [0.7,0.65,0.68],\n",
    "    'anxiety_level': [0.2,0.35,0.25],\n",
    "    'phase_type': ['exploration','lecon','defi']\n",
    "})\n",
    "template.to_csv(TEMPLATE_PATH, index=False)\n",
    "\n",
    "TEMPLATE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7137755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
